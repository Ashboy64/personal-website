---
title: 'A Lambda Calculus Interpreter in Rust'
date: '2025-06-28'
description: "Notes of functional programming and Rust's ownership system"
---

<figure>
  <img src="/personal-website/rust_lambda_calc_1/van_gogh_almond_blossoms.jpg" alt="Van Gogh's Almond Blossoms"/>
  <figcaption className="text-center">_Almond Blossoms_, Vincent Van Gogh, 1890</figcaption>
</figure>

## Introduction

The early 20th century saw the discovery of paradoxes and inconsistencies in set theory, a foundational branch of mathematics on whose results much of the rest of the subject depends. These paradoxes led mathematicians to probe the nature of logic itself and attempt to understand the limits of formal systems. The study of how logic, knowledge, and deduction could be formalized led to the invention of abstract systems of computation, and in turn, the field of computer science itself. Among the systems of computation developed to understand the nature of logic was the lambda calculus. Invented by Alonzo Church in the 1930s, the lambda calculus remains central to the theory of programming langauges, and forms the basis for modern functional languages like Haskell and OCaml.

I was first exposed to the lambda calculus during a programming languages course I took at college during fall 2024; this was probably my favorite course I ever took during my time at university. Besides theoretical topics like the lambda calculus, the course also discussed some interesting modern progamming languages such as Rust, which features a unique type system designed to guarantee memory safety without the overhead of runtime garbage collection. I thought a fun way to revisit some of the concepts we covered in this course would be to build an interpreter for the lambda calculus in Rust; this blog post discusses some of the things I learned along the way :)

You can find the source code for the interpreter here: TODO.

The next couple of sections of this post provide some background information on the lambda calculus and Rust. However, if you're unfamiliar with these topics and would like to learn more, I'd highly recommend [Stanford's CS 242 slides](https://web.stanford.edu/class/cs242/), the [Rust book](https://doc.rust-lang.org/book/), and a textbook I found called [The Implementation of Functional Programming Languages](https://simon.peytonjones.org/assets/pdfs/slpj-book-1987-searchable.pdf).

## The Lambda Calculus

### Syntax

Programs in the lambda calculus are composed of variables, function definitions, and function applications. Function definitions are written as $\lambda x. e$; this defines a function that given an input, returns $e$ where every occurrence of $x$ inside $e$ is replaced with the input that was provided. We use currying to represent functions that require multiple arguments. For example, if we have a function $f(x, y) = x$, we can write this as $\lambda x. \lambda y. x$. This defines a function that given an input $x$ returns _another_ function which, when given the input $y$, returns the value $x$. Function application is written as $e_1 e_2$ (this denotes that we apply the function defined by $e_1$ to $e_2$). Parentheses can be used to specify association. By default, we will have lambda abstractions bind to as much as the string that follows as possible, stopping only at the end of the string or a closing parenthesis that was not previously opened inside the lambda. This means that the program $(\lambda x. \lambda y. x) z$ should be interpreted as $(\lambda x. (\lambda y. x)) z$. Additionally, function application will by default be left-associative; i.e. $x y z$ should be interpreted as $(x y) z$.

Here are some example programs:

- $ (\lambda x. \lambda y. x) a b $
    - The expression in parentheses ($\lambda x. \lambda y. x$), is a function that takes in two arguments $x$ and $y$, and always returns the first argument $x$. 
    - We apply this function to $a$ and $b$, so the the program will evaluate to $a$ because $a$ was bound to $x$.

- $ \lambda f. \lambda x. x $
    - This is a function that given another function $f$ and an argument $x$, directly returns $x$.
    - Whole numbers can be encoded in the lambda calculus using 'Church encoding', which represents $n$ as a function that given a function $f$ and an input $x$, returns the result of applying $f$ to $x$ a total of $n$ times.
    - Thus, the above function corresponds to the number $0$ in Church encoding.

- $ \lambda n. \lambda f. \lambda x. f (n f x)$
    - This function takes in a whole number $n$ in Church encoding. It returns a function that takes in an argument $f$, an argument $x$, and produces as output the expression $f (n f x)$, which corresponds to applying $f$ one more time after applying it $n$ times to $f$.
    - Thus, this function computes $n+1$ given $n$; it is the 'successor' function for whole numbers.


### Computation Rules

Now that we know how to write lambda calculus programs, how do we execute them? Execution involves repeatedly applying a set of computation rules. The most important of these is _beta reduction_, which says that when we have a lambda abstraction applied to some lambda expression , we should substitute the lambda expression for every occurrence of the lambda abstraction variable in the lambda abstraction body. The parts of a program where we are able to apply beta reduction are called _redexes_. For a concrete example, consider the expression $(\lambda x. x) y$. We see that the expression is a function application, with the function $\lambda x. x$ being a lambda abstraction. Thus, this is a redex, i.e. this is an expression to which we can apply beta reduction. To do so, we will substitute the input $y$ for every occurrence of the formal parameter $x$ in the lambda abstraction body. This gives the result $y$. Since we no longer have any redexes, we have finished executing the program, with $y$ being the final result.

There is a complication with beta reduction however. Consider the following program:

$$
(\lambda x. \lambda y. x y) y
$$

If we applied beta reduction 'naively', we would produce the following string:

$$
\lambda y. y y
$$

This string represents a function that given some input y, return the output of applying y to itself. However, this is _not_ what the original program actually represents. The value $y$ to which we applied $(\lambda x. \lambda y. x y)$ was a _free variable_; it was not bound by any lambda abstraction. We can rename the formal parameter $y$ to get an equivalent program $(\lambda x. \lambda y'. x y') y$, which when reduced gives us $\lambda y' y y'$. This result has an entirely different meaning than $\lambda y. y y$, the string that we obtained when we naively applied beta reduction. 

The problem is that in the naive case, the expression we are substituting $x$ for contains free variables that are bound by inner lambda abstractions of the expression we are substituting into. This issue is known as _variable capture_. To avoid variable capture, we will use another computation rule called _alpha conversion_, which just specifies that when we detect any instances where variable capture will occur, we will rename the formal parameter to prevent it. In other words, we will make sure to detect whenever we are performing a reduction like $(\lambda x. \lambda y. x y) y$, and when we do, we will rename the lambda abstraction to be $(\lambda x. \lambda y'. x y') y$ instead.

### Reduction order

Two natural questions arise from the above discussion: (1) in what order should we reduces redexes, and (2) when do we stop applying computation rules? My implementation reduces expressions using the _call-by-name_ reduction order until the expression is in _weak head normal form_. 

Call-by-name evaluation evaluates an expression only when it is actually used. For example, let's say that we have a program that looks like $(\lambda x. e_1) e_2$. Under call-by-name, we will first substitute every instance of $x$ inside $e_1$ with $e_2$ _without_ first evaluating $e_2$; $e_2$ will only be evaluated when its value is needed for some computation inside $e_1$. This is in contrast to _call-by-value_ reduction order, which would first evaluate $e_2$ before performing substitutions into $e_1$. Another way to understand call-by-name evaluation is to see that it always reduces the uppermost, leftmost redex first while evaluating a program.

Weak-head normal form (WHNF) means that a lambda expression has no redexes along its 'left spine'. For example, consider the lambda expression $(x y) (\lambda z. z) (\lambda a. \lambda b. a b)$. We can think of this program as a tree that looks like the following:

<figure>
  <img src="/personal-website/rust_lambda_calc_1/left_spine_image.jpg" alt="Lambda calculus AST example"/>
  <figcaption className="text-center">Lambda calculus AST example</figcaption>
</figure>

The left spine of the expression consists of the nodes on the far left hand-side of the image, namely, the two function applications and the free variable $x$. Since there are no redexes along the left spine, we will consider this expression to be fully evaluated. Note however that there are still redexes that could be reduced in the right subtree of the root node. An alternative to WHNF is _normal form_, which specifies that the expression contains no more redexes whatsoever, and would require us to reduce the redexes in the right subtree as well. It's possible that normal form evaluation might cause us to evaluate some expressions that never get used, so my implementation only reduces expressions to WHNF; for a similar reason, some real-world languages like Haskell also adopt WHNF semantics.

TODO: More thorough discussion of call-by-name vs call-by-value, and WHNF vs normal form?

## Rust

Rust is a programming language that prioritizes safety and performance. Its flagship feature is its 'ownership' system, which is designed to minimize memory errors common in C/C++ (double-frees, use-after-frees, memory leaks) while avoiding the overhead of garbage collection.

When a Rust program is executing, data is produced and stored in memory (call such data _values_). In Rust, each value is associated with an _owner_. Owners can be:
- Variables (most common).
- Formal parameters of functions.
- Containers like structs, enums (Rust's name for algebraic data types), tuples.

Note that both values and owners are *dynamic*; they are produced and change as a program executes. However, Rust enforces certain ownership rules *statically* to ensure memory safety. These rules are that:
- Each value has an owner.
- There exists only one owner of a value at any point during execution.
- When program execution proceeds to a point where, for some value, its owner is no longer associated with an indentifier in the code (i.e. the value's owner has gone out of scope), that value is deallocated.

So far, this sounds pretty simple -- we deallocate values when they go out of scope. The catch is that values might be _aliased_; multiple variables might refer to the same data during execution. We need a way to pick one of them as the unique owner, and ensure that it is not possible to continue to use the other variables referring to that value when the owner goes out of scope.

To resolve this, the programmer tells the Rust compiler which variable / function parameter / container is the unique owner of some value in the program they write. The other variables through which that value can be accessed are _references_. References _borrow_ values from their owners, and when a reference goes out-of-scope, the underlying value is *not* deallocated (deallocation only happens when the owner goes out-of-scope). References are basically pointers, but Rust's ownership / borrowing rules guarantee that they will always point to valid memory. 

To do this, each reference is associated with a _lifetime_. A reference's lifetime is the region of code where it can be used. Rust's borrow checker infers lifetimes for all references in a program and makes sure that every reference's lifetime does not exceed the lifetime of the value's owner (programmers might have to specify some lifetime information manually though, particularly at function calls and when defining structs).


## Interpreter Overview

When users run the interpreter, they specify a single source code file to run the interpreter on. The interpreter reads the file into a string, lexes it into a vector of tokens, parses the vector into an AST, and finally executes the AST. The execution engine applies reduction rules in lazy / call-by-name order until the given AST is in WHNF. 

A common optimization in functional languages is to implement _sharing_, which tries to reduce the number of times we need to evaluate the same expression. The key idea of sharing is that when we perform substitutions, instead of substituting each instance of a formal parameter with a new deepcopy of its associated expression, we instead replace all instances of the formal parameter with pointers to a single copy of its associated expression. Now, if we ever evaluate any single instance of the formal parameter, we can reuse the results of that evaluation anytime that it's needed instead of re-evaluating the expression from scratch.

For example, consider the program $ (\lambda f. \lambda x. (f x) (f x)) ((\lambda a. a) (\lambda b. b)) x $. Under regular lazy evaluation, we would apply computation rules like so:

$$
\begin{align*}
(\lambda f. \lambda x. (f x) (f x)) & ((\lambda a. a) (\lambda b. b)) x \\
&\rightarrow (\lambda x. (((\lambda a. a) (\lambda b. b)) x) (((\lambda a. a) (\lambda b. b)) x) ) x \\
&\rightarrow (((\lambda a. a) (\lambda b. b)) x) (((\lambda a. a) (\lambda b. b)) x) \\
&\rightarrow ((\lambda b. b) x) (((\lambda a. a) (\lambda b. b)) x) \\
&\rightarrow x (((\lambda a. a) (\lambda b. b)) x)
\end{align*}
$$

With sharing, the evaluation of the program would look like this:

$$
\begin{align*}
(\lambda f. \lambda x. (f x) (f x)) & ((\lambda a. a) (\lambda b. b)) x \\
&\rightarrow (\lambda x. (((\lambda a. a) (\lambda b. b)) x) (((\lambda a. a) (\lambda b. b)) x) ) x \\
&\rightarrow (((\lambda a. a) (\lambda b. b)) x) (((\lambda a. a) (\lambda b. b)) x) \\
&\rightarrow ((\lambda b. b) x) ((\lambda b. b) x) \\
&\rightarrow x ((\lambda b. b) x)
\end{align*}
$$

As you can see, the evaluation of $(\lambda a. a) (\lambda b. b)$ is shared between both instances of $f$, so in the final output, $f$ is evaluated to the value $\lambda b. b$ in the argument to $x$. 

Sharing does slightly break the semantics of WHNF because it now might look like some extra expressions were evaluated as compared to true WHNF execution. This is not an issue for two reasons:
- The Church-Rosser theorem states that the lambda calclus is confluent, i.e., given that $e_1 \rightarrow* e_2$ and $e_1 \rightarrow* e_3$, there must exist some $e_4$ such that $e_2 \rightarrow* e_4$ and $e_3 \rightarrow* e_4$. Thus, even though the exact output strings produced with sharing may look different from those produced by exact WHNF evaluation, they will still be in some sense 'equivalent'.
- Sharing provides significant efficiency benefits when evaluating lambda calclus programs, and these gains are large enough to warrant violating the 'pure' definition of WHNF.

Despite its benefits, as part of my first pass on the project, I chose not to implement sharing as I wanted to focus on keeping the implementation as conceptually simple as possible. For the same reason, I didn't implement another optimization called De Bruijn indices which would remove the need for alpha-conversion. Implementing sharing and De Bruijn indices are two important extensions I hope to make on this project in the coming months.


## Language Syntax

Here's a short program that demonstrates the key syntax of the language that the interpreter accepts:

```
// Example of Church encoding of whole numbers.

def zero = \f. \x. x;
def succ = \n. \f. \x. f (n f x);
def one = succ zero;
def two = succ one;

eval zero f x;
eval one f x;
eval two f x;
```

The interpreter will produce this string as output:

```
x
f (zero f x)
f (one f x)
```

As you can see, users can add comments with '//'. The program consists of a sequence of `def` statements and `eval` statements. Eval statements are the lambda expressions that the interpreter will actually execute. Def statements essentially function as macros. As the interpreter executes an eval statement, if it ever needs to evaluate a variable that looks like a free variable, it will first check if there is a def statement that maps that free variable name to some sub-expression, and if so, the interpreter will substitute that sub-expression into the eval statement and continue execution.


## Lexical Analysis, Parsing, and Program AST Representation

The interpreter has three main stages: lexical analysis, parsing, and code execution. As part of lexical analysis, the input program string is turned into a sequence of tokens, each of which belong to some 'token class'. Some token classes include 'whitespace', 'comment', 'identifier', and 'lambda'. The implementation of the lexer is fairly simple. Each token class is associated with a regex, and given an input string, the lexer emits the token which matches the greatest number of characters beginning from the current start of the string. After emitting this token, the starting index is incremented, and the process repeats.

The sequence of tokens produced by the lexer is then consumed by the parser. The parser converts the token sequence into a data structure called an abstract syntax tree (AST). This AST is later used during code execution.

I chose to represent the program AST for a lambda expression as a collection of heap-allocated 'ExprNodes', with each node having a unique pointer to its descendants in the AST. Here is a snippet of code from the codebase defining these nodes:

```
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum ExprNode {
    FnDef {
        formal_param: String,
        fn_body: Box<ExprNode>,
    },
    FnApp {
        fn_body: Box<ExprNode>,
        actual_arg: Box<ExprNode>,
    },
    Var {
        var_name: String,
    },
}
```

This program representation is extremely simple. Using unique Box pointers is a natural fit in this case since I chose not to implement sharing, but when I do implement this, I'll likely have to switch to using Rust's `Rc<T>` type instead.

The parser is a simple recursive descent parser (a fancy way to say backtracking search on grammar production rules). You can read more about recursive descent parsing in [Stanford's CS 143 slides](), or the Dragon Book[]().

Determining the exact grammar to implement in the parser took a little bit of iterating. Here's a first attempt :

$$
e \rightarrow \lambda x. e \ | \ e_1 e_2 \ | \ (e) \ | \ x
$$

Unfortunately this grammar is ambiguous. There are two problems:
- Consider the expression $\lambda x. x \lambda y. y$. Should this be parsed as $(\lambda x. x) (\lambda y. y)$, or as $\lambda x. (x \lambda y. y)$? In other words, what is the scoping for lambda abstractions with respect to the string that follows?
- Consider the expression $x y z$. Should this be parsed as $(x y) z$ or $x (y z)$? In other words, what is the association for function application?

As mentioned before, we want to have lambdas bind for as much of the string that follows them as possible, and we want to have function application be left-associative. Another constraint to keep in mind is that since we are using a recursive descent parser, we need to make sure that the grammar is not left-recursive.

Here's the grammar I ended up implementing:

$$
\begin{align*}
&e \rightarrow \lambda x. e \ | \ \text{app}  \\
&\text{app} \rightarrow \text{atom} \ | \ \text{atom} \; \text{e} \\
&\text{atom} \rightarrow (e) \ | \ v \\
\end{align*}
$$

To resolve the ambiguity with lambda abstractions, note that we want to invalidate the derivation $e \rightarrow e \ e \rightarrow \lambda x. e \ e$ in our original grammar, because in this case the lambda abstraction does not bind for as much of the string as possible. The above grammar does this by introducing a new nonterminal called $\text{non\_lam}$, which represents expressions that do not start with a lambda abstraction. When we now have function applications, the leftmost nonterminal is always going to be a $\text{non\_lam}$

## Battles with the Borrow Checker







